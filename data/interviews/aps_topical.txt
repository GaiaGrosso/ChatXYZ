Can a Computer Devise a Theory of Everything?” This provocative question, posed in the title of a New York Times article from 2020, anticipates a (dystopian?) future where theoretical physicists might be replaced by their robotic counterparts. Whether or not such a future will ever come to pass is debatable, but already today, questions like these reflect the immense promise of machine learning (ML) to advance the frontiers of the physical sciences. On the one hand, this kind of question is maddening, since the present-day algorithms driving ML innovations are far removed from those employed by human physicists. On the other hand, this kind of question is inspiring, since it forces us to reflect on what aspects of the scientific process could be systematized and automated. Robot futures aside, ML has already had an irreversible impact on my field of high-energy physics (HEP), as represented by the growing catalog of papers on the topic. A few years ago, one could legitimately question the benefits of modern ML techniques given the strong performance of traditional methods. Now, one can legitimately question the benefits of solely relying on traditional methods given the dramatic gains seen from ML applications. This is particularly true when one can infuse “physics intelligence” into artificial intelligence, such that decisions made by machines are underpinned by principles, best practices, and domain knowledge from the physical sciences. As Director of the NSF Institute for Artificial Intelligence and Fundamental Interactions (IAIFI), I have a front-row seat to the many innovative ways that physicists are incorporating ML into their research. From calculating the properties of nuclei from first principles, to inferring the nature of dark matter from astrophysical observations, to improving the operations of large-scale physics experiments, ML is having a transformational impact across theoretical, experimental, and computational physics. Equally exciting, physics techniques from statistical mechanics and quantum field theory are being used to understand the inner workings of ML systems. This fruitful dialogue between ML and physics is unlikely to slow down anytime soon. When I was an undergraduate student, multivariate calculus, differential equations, and linear algebra were viewed as the basic mathematical prerequisites for physics research. Today, given the ubiquity of rich data sets in physics, it is equally important for physics students to learn the foundations of probability, statistics, computational methods, and data analysis. In recognition of the growing importance of data science for physics research, we recently launched an Interdisciplinary Ph.D. in Physics, Statistics, and Data Science at MIT. The first recipients of this degree have written Ph.D. theses in fields ranging from particle physics, to plasma science, to gravitational waves, all enabled by ML in some form. To maintain this exciting momentum at the intersection of physics and ML, I believe we need to expand the space of ML in three complementary directions. Expanding the space of ML methods. Much of the buzz in ML is around “deep learning”, specifically supervised learning with multi-layer feed-forward neural networks. Such techniques have shown transformational potential in physics, but we have a larger opportunity to leverage analysis strategies from various areas of mathematics, statistics, and computer science. My recent HEP research leverages techniques from optimal transport (OT), by using the “earth mover’s distance” to determine whether two collider events are similar or dissimilar. I heard about OT from an MIT colleague working on computational geometry, which is usually not placed under the banner of ML. Instead of limiting the scope of ML to specific tools, though, I think it is more fruitful to view ML as a big umbrella that includes any algorithm that helps us make sense of scientific data sets. This framing inspires us to look beyond neural networks and find the best method to solve the problem at hand. Expanding the space of ML applications. Much of the recent progress in ML for physics has targeted low-hanging fruit, where off-the-shelf ML algorithms have replaced existing elements of the analysis pipeline. For HEP applications in particular, deep learning is supplanting traditional strategies for object reconstruction and identification. As the community gains more expertise in ML methods, we have an opportunity to translate more aspects of physics into a computational language and thereby enhance the way we manipulate theoretical and experimental data. Two key tools used in physics are symbolic computation (e.g. for theoretical calculations) and numerical simulation (e.g. for experimental modeling). To incorporate these tools into an ML-based pipeline, physicists are exploring the use of symbolic regression and differentiable programming, respectively. These developments force us to go beyond off-the-shelf ML tools and develop custom solutions to integrate ML into physics frameworks. Expanding the space of ML career pathways. There is a vibrant community in ML for physics, especially among early-career researchers. The members of this community straddle the traditional boundaries between theoretical physicist, experimental physicist, statistician, and data scientist. Through the IAIFI Fellowship program, a group of talented postdocs has assembled in the Boston area to collaborate at the multi-disciplinary intersection between physics and ML. To continue to recruit and retain the most promising talent in this area, we have an opportunity to create new career pathways at the physics/ML interface, both in academia and in industry. Researchers in physics are united in the quest to understand nature. Researchers with dual training in physics and ML have the skills necessary to tackle a broader universe of questions in and beyond the physical sciences. By expanding the space of ML in physics, we can increase the discovery potential of physics experiments, invigorate theoretical physics research, and demonstrate the value of physics training to address broader societal challenges. We are likely far away from a future where a computer could devise a theory of everything without human input. We are likely closer to a (utopian?) future where a group of humans with interdisciplinary training could devise a theory of everything with help from ML-enabled computation.