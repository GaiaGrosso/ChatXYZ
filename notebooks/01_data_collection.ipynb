{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3513cde2-29d4-4688-b29d-200afbfd13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/openai/whisper.git\n",
    "# ! pip install pytube\n",
    "# ! pip install pypdf langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f44e2c-9a64-4386-a636-5ddaed26e22b",
   "metadata": {},
   "source": [
    "## 1. Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13ea1811-93b7-4c00-b08e-f3f210d14a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from utils.arxiv_utils import get_inspire_hep_papers, extract_arxiv_ids, download_arxiv_source, delete_files_except_extensions, get_filenames_with_extensions\n",
    "from utils.db_utils import update_dataframe\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09f6f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = '../data/papers/'\n",
    "db_dir = '../data/db/'\n",
    "txt_dir = '../data/interviews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1147bcaf-6167-47ba-b12f-49c83aed50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    os.makedirs(pdf_dir)\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{pdf_dir}' already exists\")\n",
    "\n",
    "# Get papers from INSPIRE-HEP\n",
    "papers = get_inspire_hep_papers(\"Jesse.Thaler.1\") \n",
    "\n",
    "# Extract arXiv IDs from papers\n",
    "arxiv_ids = extract_arxiv_ids(papers)\n",
    "\n",
    "# Download papers (sources if available, otherwise PDFs)\n",
    "[download_arxiv_source(arxiv_id, output_dir=pdf_dir) for arxiv_id in tqdm(arxiv_ids)];  \n",
    "\n",
    "# Delete all files except PDFs and TeX files; load relevant files list\n",
    "delete_files_except_extensions(pdf_dir, ['.pdf', '.tex'])\n",
    "filenames = get_filenames_with_extensions(pdf_dir, ['.tex', '.pdf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff08afe-449a-4f4d-abbc-5444c6da6f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with file hep-ph_0604192.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:16<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "source_type = []\n",
    "text = []\n",
    "\n",
    "for i, file in enumerate(tqdm(filenames)):\n",
    "    try:\n",
    "        if os.path.splitext(file)[-1] == '.pdf':\n",
    "            loader = PyPDFLoader(\"{}/{}\".format(pdf_dir, file))\n",
    "            pages = loader.load_and_split()\n",
    "            text.append(''.join([page.page_content for page in pages]))\n",
    "            source_type.append(\"paper\")\n",
    "        elif os.path.splitext(file)[-1] == '.tex':\n",
    "            with open(\"{}/{}\".format(pdf_dir, file), 'r', encoding='iso-8859-1') as f:\n",
    "                text.append(f.read())\n",
    "                source_type.append(\"paper\")\n",
    "    except:\n",
    "        print(\"Error with file {}\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2cf145-2efe-47ae-b0ae-f46450ce4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [source_type, text]\n",
    "\n",
    "# Transpose the data to have the inner lists as rows\n",
    "transposed_data = list(map(list, zip(*data)))\n",
    "\n",
    "# Column names for the DataFrame\n",
    "columns = ['source_type','text']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(transposed_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1adee0-cab0-40c4-b4d9-370a327de4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataframe\n",
    "df = update_dataframe('{}/df_text.csv'.format(db_dir), df)\n",
    "df.to_csv('{}/df_text.csv'.format(db_dir), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b2865-d315-41b1-9c26-531dc2301412",
   "metadata": {},
   "source": [
    "## 2. YouTube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64be795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import whisper\n",
    "# import pytube\n",
    "# from pathlib import Path\n",
    "# import subprocess\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918b7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_dir = \"../data/videos/\"\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(videos_dir)\n",
    "# except FileExistsError:\n",
    "#     print(f\"Directory '{videos_dir}' already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd8e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get whisper model; download weights if necessary\n",
    "# whisper_model = whisper.load_model(\"tiny.en\").to('cpu')\n",
    "# options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "\n",
    "# url = \"https://www.youtube.com/watch?v=dqxdPNzBY0I\"\n",
    "# pytube_vid = pytube.YouTube(url)\n",
    "# video_path_local = Path(videos_dir).resolve() / (pytube_vid.video_id+\".mp4\")\n",
    "# pytube_vid.streams.filter(type=\"audio\", mime_type=\"audio/mp4\", abr=\"48kbps\").first().download(output_path=video_path_local.parent, filename=video_path_local.name)\n",
    "# video_path_local = video_path_local.with_suffix(\".wav\")\n",
    "# result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n",
    "# transcription = whisper.transcribe(whisper_model, str(video_path_local))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741ca70-fb4d-4aca-ac61-49359e39aae4",
   "metadata": {},
   "source": [
    "## 3. Interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "598980b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = get_filenames_with_extensions(txt_dir, ['.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "363b76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 7292.33it/s]\n"
     ]
    }
   ],
   "source": [
    "source_type = []\n",
    "text = []\n",
    "\n",
    "for i, file in enumerate(tqdm(filenames)):\n",
    "    try:\n",
    "        with open(\"{}/{}\".format(txt_dir, file), 'r') as f:\n",
    "            text.append(f.read())\n",
    "            source_type.append(\"interview\")\n",
    "    except:\n",
    "        print(\"Error with file {}\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a7e4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [source_type, text]\n",
    "\n",
    "# Transpose the data to have the inner lists as rows\n",
    "transposed_data = list(map(list, zip(*data)))\n",
    "\n",
    "# Column names for the DataFrame\n",
    "columns = ['source_type','text']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(transposed_data, columns=columns)\n",
    "\n",
    "# Update dataframe\n",
    "df = update_dataframe('{}/df_text.csv'.format(db_dir), df)\n",
    "df.to_csv('{}/df_text.csv'.format(db_dir), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "600ca3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source_type</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>paper</td>\n",
       "      <td>%\\RequirePackage{lineno}\\n\\documentclass[aps,p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>paper</td>\n",
       "      <td>\\documentclass[aps,prd,floatfix,preprintnumber...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>paper</td>\n",
       "      <td>\\documentclass[letterpaper,11pt]{article}\\n\\pd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>paper</td>\n",
       "      <td>\\documentclass[11pt,letterpaper]{article}\\n\\pd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paper</td>\n",
       "      <td>\\documentclass[aps,twocolumn,nofootinbib,super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>interview</td>\n",
       "      <td>The Future of Particle Physics is \"Open\"\\nGues...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>interview</td>\n",
       "      <td>Can we trust physics decisions made by machine...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>interview</td>\n",
       "      <td>Can a Computer Devise a Theory of Everything?”...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>interview</td>\n",
       "      <td>Jesse Thaler is a theoretical particle physici...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>interview</td>\n",
       "      <td>For decades, particle colliders have exposed t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>130 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    source_type                                               text\n",
       "0         paper  %\\RequirePackage{lineno}\\n\\documentclass[aps,p...\n",
       "1         paper  \\documentclass[aps,prd,floatfix,preprintnumber...\n",
       "2         paper  \\documentclass[letterpaper,11pt]{article}\\n\\pd...\n",
       "3         paper  \\documentclass[11pt,letterpaper]{article}\\n\\pd...\n",
       "4         paper  \\documentclass[aps,twocolumn,nofootinbib,super...\n",
       "..          ...                                                ...\n",
       "125   interview  The Future of Particle Physics is \"Open\"\\nGues...\n",
       "126   interview  Can we trust physics decisions made by machine...\n",
       "127   interview  Can a Computer Devise a Theory of Everything?”...\n",
       "128   interview  Jesse Thaler is a theoretical particle physici...\n",
       "129   interview  For decades, particle colliders have exposed t...\n",
       "\n",
       "[130 rows x 2 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('{}/df_text.csv'.format(db_dir))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb533fd8-f08d-4c00-8d35-d1b28da4552d",
   "metadata": {},
   "source": [
    "## 4. Website/CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a982fa31-d5ed-437d-b96c-f04e422ed4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "047261b5-9d87-493c-941f-2c84237f4339",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1e04d17-904e-4536-bfe4-6fd02859c804",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d33c292f-638f-46d7-93e4-72e062f9331f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "533fcfac-a994-4622-8038-cd64cf3f456b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
