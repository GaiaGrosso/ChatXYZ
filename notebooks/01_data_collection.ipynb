{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3513cde2-29d4-4688-b29d-200afbfd13d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install git+https://github.com/openai/whisper.git\n",
    "# ! pip install pytube\n",
    "# ! pip install pypdf langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1f44e2c-9a64-4386-a636-5ddaed26e22b",
   "metadata": {},
   "source": [
    "## 1. Papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "13ea1811-93b7-4c00-b08e-f3f210d14a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "from utils.arxiv_utils import get_inspire_hep_papers, extract_arxiv_ids, download_arxiv_source, delete_files_except_extensions, get_filenames_with_extensions\n",
    "from utils.db_utils import update_dataframe\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09f6f064",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_dir = '../data/papers/'\n",
    "db_dir = '../data/db/'\n",
    "txt_dir = '../data/interviews/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1147bcaf-6167-47ba-b12f-49c83aed50fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "try:\n",
    "    os.makedirs(pdf_dir)\n",
    "except FileExistsError:\n",
    "    print(f\"Directory '{pdf_dir}' already exists\")\n",
    "\n",
    "# Get papers from INSPIRE-HEP\n",
    "papers = get_inspire_hep_papers(\"Jesse.Thaler.1\") \n",
    "\n",
    "# Extract arXiv IDs from papers\n",
    "arxiv_ids = extract_arxiv_ids(papers)\n",
    "\n",
    "# Download papers (sources if available, otherwise PDFs)\n",
    "[download_arxiv_source(arxiv_id, output_dir=pdf_dir) for arxiv_id in tqdm(arxiv_ids)];  \n",
    "\n",
    "# Delete all files except PDFs and TeX files; load relevant files list\n",
    "delete_files_except_extensions(pdf_dir, ['.pdf', '.tex'])\n",
    "filenames = get_filenames_with_extensions(pdf_dir, ['.tex', '.pdf'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ff08afe-449a-4f4d-abbc-5444c6da6f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/125 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error with file hep-ph_0604192.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 125/125 [00:16<00:00,  7.56it/s]\n"
     ]
    }
   ],
   "source": [
    "source_type = []\n",
    "text = []\n",
    "\n",
    "for i, file in enumerate(tqdm(filenames)):\n",
    "    try:\n",
    "        if os.path.splitext(file)[-1] == '.pdf':\n",
    "            loader = PyPDFLoader(\"{}/{}\".format(pdf_dir, file))\n",
    "            pages = loader.load_and_split()\n",
    "            text.append(''.join([page.page_content for page in pages]))\n",
    "            source_type.append(\"paper\")\n",
    "        elif os.path.splitext(file)[-1] == '.tex':\n",
    "            with open(\"{}/{}\".format(pdf_dir, file), 'r', encoding='iso-8859-1') as f:\n",
    "                text.append(f.read())\n",
    "                source_type.append(\"paper\")\n",
    "    except:\n",
    "        print(\"Error with file {}\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2cf145-2efe-47ae-b0ae-f46450ce4f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [source_type, text]\n",
    "\n",
    "# Transpose the data to have the inner lists as rows\n",
    "transposed_data = list(map(list, zip(*data)))\n",
    "\n",
    "# Column names for the DataFrame\n",
    "columns = ['source_type','text']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(transposed_data, columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a1adee0-cab0-40c4-b4d9-370a327de4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update dataframe\n",
    "df = update_dataframe('{}/df_text.csv'.format(db_dir), df)\n",
    "df.to_csv('{}/df_text.csv'.format(db_dir), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b2865-d315-41b1-9c26-531dc2301412",
   "metadata": {},
   "source": [
    "## 2. YouTube videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64be795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm\n",
    "# import whisper\n",
    "# import pytube\n",
    "# from pathlib import Path\n",
    "# import subprocess\n",
    "# import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "918b7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_dir = \"../data/videos/\"\n",
    "\n",
    "# try:\n",
    "#     os.makedirs(videos_dir)\n",
    "# except FileExistsError:\n",
    "#     print(f\"Directory '{videos_dir}' already exists\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbd8e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get whisper model; download weights if necessary\n",
    "# whisper_model = whisper.load_model(\"tiny.en\").to('cpu')\n",
    "# options = whisper.DecodingOptions(language=\"en\", without_timestamps=True)\n",
    "\n",
    "# url = \"https://www.youtube.com/watch?v=dqxdPNzBY0I\"\n",
    "# pytube_vid = pytube.YouTube(url)\n",
    "# video_path_local = Path(videos_dir).resolve() / (pytube_vid.video_id+\".mp4\")\n",
    "# pytube_vid.streams.filter(type=\"audio\", mime_type=\"audio/mp4\", abr=\"48kbps\").first().download(output_path=video_path_local.parent, filename=video_path_local.name)\n",
    "# video_path_local = video_path_local.with_suffix(\".wav\")\n",
    "# result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n",
    "# transcription = whisper.transcribe(whisper_model, str(video_path_local))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1741ca70-fb4d-4aca-ac61-49359e39aae4",
   "metadata": {},
   "source": [
    "## 3. Interviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "598980b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = get_filenames_with_extensions(txt_dir, ['.txt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "363b76c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:00<00:00, 7292.33it/s]\n"
     ]
    }
   ],
   "source": [
    "source_type = []\n",
    "text = []\n",
    "\n",
    "for i, file in enumerate(tqdm(filenames)):\n",
    "    try:\n",
    "        with open(\"{}/{}\".format(txt_dir, file), 'r') as f:\n",
    "            text.append(f.read())\n",
    "            source_type.append(\"interview\")\n",
    "    except:\n",
    "        print(\"Error with file {}\".format(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a7e4943",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [source_type, text]\n",
    "\n",
    "# Transpose the data to have the inner lists as rows\n",
    "transposed_data = list(map(list, zip(*data)))\n",
    "\n",
    "# Column names for the DataFrame\n",
    "columns = ['source_type','text']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(transposed_data, columns=columns)\n",
    "\n",
    "# Update dataframe\n",
    "df = update_dataframe('{}/df_text.csv'.format(db_dir), df)\n",
    "df.to_csv('{}/df_text.csv'.format(db_dir), index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb533fd8-f08d-4c00-8d35-d1b28da4552d",
   "metadata": {},
   "source": [
    "## 4. Website/CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a982fa31-d5ed-437d-b96c-f04e422ed4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "def get_clean_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "        # Remove script and style tags\n",
    "        for script in soup(['script', 'style']):\n",
    "            script.decompose()\n",
    "\n",
    "        # Get text and remove leading/trailing whitespaces\n",
    "        text = soup.get_text(strip=True)\n",
    "\n",
    "        # Replace multiple whitespaces with a single space\n",
    "        clean_text = ' '.join(text.split())\n",
    "\n",
    "        return clean_text\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error while fetching URL {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "047261b5-9d87-493c-941f-2c84237f4339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6/6 [00:01<00:00,  5.23it/s]\n"
     ]
    }
   ],
   "source": [
    "websites = [\"https://jthaler.net/group\", \"https://jthaler.net/research\", \"https://jthaler.net/engagement\", \"https://jthaler.net/faq\", \"https://jthaler.net/cv\", \"https://jthaler.net/contact\"]\n",
    "text_website = [get_clean_text(website) for website in tqdm(websites)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f1e04d17-904e-4536-bfe4-6fd02859c804",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [len(text_website) * [\"website\"], text_website]\n",
    "\n",
    "# Transpose the data to have the inner lists as rows\n",
    "transposed_data = list(map(list, zip(*data)))\n",
    "\n",
    "# Column names for the DataFrame\n",
    "columns = ['source_type','text']\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame(transposed_data, columns=columns)\n",
    "\n",
    "# Update dataframe\n",
    "df = update_dataframe('{}/df_text.csv'.format(db_dir), df)\n",
    "df.to_csv('{}/df_text.csv'.format(db_dir), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d0c69b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
